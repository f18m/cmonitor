#!/usr/bin/python3

#
# cmonitor_chart.py
# Originally based on the "njmonchart_aix_v7.py" from Nigel project: http://nmon.sourceforge.net/
#
# Author: Francesco Montorsi
# Created: April 2019
#

import sys
import json
import gzip
import datetime
import zlib
import binascii
import textwrap
import argparse
import getopt
import os
import time

# =======================================================================================================
# CONSTANTS
# =======================================================================================================

VERSION_STRING = "__RPM_VERSION__-__RPM_RELEASE__"

GRAPH_SOURCE_DATA_BAREMETAL = 1
GRAPH_SOURCE_DATA_CGROUP = 2

GRAPH_TYPE_AREA_CHART = 1
GRAPH_TYPE_BUBBLE_CHART = 2

SAVE_DEFLATED_JS_DATATABLES = True
JS_INDENT_SIZE = 2

# see https://developers.google.com/chart/interactive/docs/reference#dateformat
# the idea is that cmonitor_chart will most likely be used to explore short time intervals
# so that day/month/year part is not useful, just the time is useful; in tooltip we also
# reach millisec accuracy:
X_AXIS_DATEFORMAT = "HH:mm:ss"
TOOLTIP_DATEFORMAT = "HH:mm:ss.SSS z"


# =======================================================================================================
# GLOBALs
# =======================================================================================================

verbose = False
g_num_generated_charts = 1
g_next_graph_need_stacking = 0
g_datetime = "localtz"  # can be changed to "UTC" with --utc; FIXME currently we always use just UTC, never localtz...

# =======================================================================================================
# GoogleChartsTimeSeries
# =======================================================================================================


class GoogleChartsTimeSeries(object):
    """
    GoogleChartsTimeSeries is a (N+1)xM table of
       t_1;Y1_1;Y2_1;...;YN_1
       t_2;Y1_2;Y2_2;...;YN_2
       ...
       t_M;Y1_M;Y2_M;...;YN_M
    data points for a GoogleCharts graph that is representing the evolution of N quantities over time
    """

    def __init__(self, column_names):
        self.column_names = column_names  # must be a LIST of strings
        self.rows = []  # list of lists with values

    def ISOdatetimeToJSDate(self, date):
        """convert ISO datetime strings like
          "2017-08-21T20:12:30.123"
        to strings like:
          "Date(2017,8,21,20,12,30,123000)"
        which are the datetime representation suitable for JS GoogleCharts, see
        https://developers.google.com/chart/interactive/docs/datesandtimes
        """
        dateAsPythonObj = datetime.datetime.strptime(date, "%Y-%m-%dT%H:%M:%S.%f")

        return "Date(%d,%d,%d,%d,%d,%d,%d)" % (
            dateAsPythonObj.year,
            dateAsPythonObj.month,
            dateAsPythonObj.day,
            dateAsPythonObj.hour,
            dateAsPythonObj.minute,
            dateAsPythonObj.second,
            dateAsPythonObj.microsecond / 1000,  # NOTE: the JavaScript Date() object wants milliseconds
        )

    def addRow(self, row_data_list):
        assert len(row_data_list) == len(self.column_names)

        # convert first column to a GoogleCharts-compatible datetime:
        row_data_list[0] = self.ISOdatetimeToJSDate(row_data_list[0])
        self.rows.append(row_data_list)

    def getRow(self, index):
        return self.rows[index]

    def getListColumnNames(self):
        return self.column_names

    def getNumDataSeries(self):
        # assuming first column is the timestamp, the number of "data series"
        # present in this table is all remaining columns
        assert len(self.column_names) >= 2
        return len(self.column_names) - 1

    def getDataSeriesIndexByName(self, column_name):
        assert column_name != self.column_names[0]  # the first column is not a "data serie", it's the timestamp column!
        try:
            col_idx = self.column_names.index(column_name)
            assert col_idx >= 1
            return col_idx - 1  # the first data serie, with index 0, is the column immediately after the timestamp column
        except ValueError:
            # column name not found
            return -1

    def writeTo(self, file):
        for r in self.rows:
            # assume first column is always the timestamp:
            row_text = "['Date(%s)'," % r[0]
            row_text += ",".join(str(x) for x in r[1:])
            row_text += "],\n"
            file.write(row_text)

    def toJSONForJS(self):
        ret = "[["  # start 2D JSON array

        # convert 1st column:
        assert self.column_names[0] == "Timestamp"
        ret += '{"type":"datetime","label":"Datetime"},'

        # convert all other columns:
        for colName in self.column_names[1:]:
            ret += '"' + colName + '",'
        ret = ret[:-1]

        # separe first line; start conversion of actual table data:
        ret += "],"

        data = json.dumps(self.rows, separators=(",", ":"))
        data = data[1:]

        return ret + data

    def toDeflatedJSONBase64Encoded(self):
        """Returns this table in JSON format (for JS), deflated using zlib, and represented as a Base64-encoded ASCII string"""
        json_string = self.toJSONForJS()
        json_compressed_bytearray = zlib.compress(json_string.encode(), 9)

        ret = str(binascii.b2a_base64(json_compressed_bytearray))
        return ret[1:]

    def toGoogleChartTable(self, graphName):
        """Writes in the given file the JavaScript GoogleCharts object representing this table"""
        ret_string = ""
        if SAVE_DEFLATED_JS_DATATABLES:
            # to reduce the HTML size save the deflated, serialized JSON of the 2D JS array:
            ret_string += "var deflated_data_base64_%s = %s;\n" % (
                graphName,
                self.toDeflatedJSONBase64Encoded(),
            )

            # then convert it base64 -> JS binary string
            ret_string += "var deflated_data_binary_%s = window.atob(deflated_data_base64_%s);\n" % (graphName, graphName)

            # now inflate it in the browser using "pako" library (https://github.com/nodeca/pako)
            ret_string += "var inflated_data_%s = JSON.parse(pako.inflate(deflated_data_binary_%s, { to: 'string' }));\n" % (graphName, graphName)
        else:
            ret_string += "var inflated_data_%s = %s;\n" % (
                graphName,
                self.toJSONForJS(),
            )

        # finally create the GoogleCharts table from it:
        ret_string += "var data_%s = google.visualization.arrayToDataTable(inflated_data_%s);\n" % (graphName, graphName)

        # add DateFormatter to use custom formatting of the 1st column (like everywhere else we assume first column is the timestamp)
        ret_string += "var date_formatter = new google.visualization.DateFormat({pattern: '%s'});\n" % (TOOLTIP_DATEFORMAT)
        ret_string += "date_formatter.format(data_%s, 0);\n\n" % (graphName)

        return ret_string


# =======================================================================================================
# GoogleChartsGenericTable
# =======================================================================================================


class GoogleChartsGenericTable(object):
    """
    This is the NxM table of
       Y1_1;Y2_1;...;YN_1
       ...
       Y1_M;Y2_M;...;YN_M
    data points for a GoogleCharts graph for M different objects characterized by N features.
    This class is useful to create graphs which are NOT related to a measurement that evolves over TIME.

    Currently this class is used only for the generation of bubble charts, which are, by their nature,
    suited to represent relationships among different features (in our case total IO, memory and CPU usage)
    """

    def __init__(self, column_names):
        self.column_names = column_names  # must be a LIST of strings
        self.rows = []  # list of lists with values

    def addRow(self, row_data_list):
        assert len(row_data_list) == len(self.column_names)
        self.rows.append(row_data_list)

    def getRow(self, index):
        return self.rows[index]

    def getListColumnNames(self):
        return self.column_names

    def getNumDataSeries(self):
        # assuming first column is the timestamp, the number of "data series"
        # present in this table is all remaining columns
        return len(self.column_names) - 1

    def writeTo(self, file):
        for r in self.rows:
            file.write(",".join(r))

    def toJSONForJS(self):
        ret = "[["  # start 2D JSON array

        # convert all other columns:
        for colName in self.column_names:
            ret += '"' + colName + '",'
        ret = ret[:-1]

        # separe first line; start conversion of actual table data:
        ret += "],"

        data = json.dumps(self.rows, separators=(",", ":"))
        data = data[1:]

        return ret + data

    def toDeflatedJSONBase64Encoded(self):
        """Returns this table in JSON format (for JS), deflated using zlib, and represented as a Base64-encoded ASCII string"""
        json_string = self.toJSONForJS()
        json_compressed_bytearray = zlib.compress(json_string.encode(), 9)

        ret = str(binascii.b2a_base64(json_compressed_bytearray))
        return ret[1:]

    def toGoogleChartTable(self, graphName):
        """Writes in the given file the JavaScript GoogleCharts object representing this table"""
        ret_string = ""
        if SAVE_DEFLATED_JS_DATATABLES:
            # to reduce the HTML size save the deflated, serialized JSON of the 2D JS array:
            ret_string += "var deflated_data_base64_%s = %s;\n" % (
                graphName,
                self.toDeflatedJSONBase64Encoded(),
            )

            # then convert it base64 -> JS binary string
            ret_string += "var deflated_data_binary_%s = window.atob(deflated_data_base64_%s);\n" % (graphName, graphName)

            # now inflate it in the browser using "pako" library (https://github.com/nodeca/pako)
            ret_string += "var inflated_data_%s = JSON.parse(pako.inflate(deflated_data_binary_%s, { to: 'string' }));\n" % (graphName, graphName)
        else:
            ret_string += "var inflated_data_%s = %s;\n" % (
                graphName,
                self.toJSONForJS(),
            )

        # finally create the GoogleCharts table from it:
        ret_string += "var data_%s = google.visualization.arrayToDataTable(inflated_data_%s);\n" % (graphName, graphName)
        return ret_string


# =======================================================================================================
# GoogleChartsGraph
# =======================================================================================================


class GoogleChartsGraph:
    """
    This is a simple object that can generate a JavaScript snippet (to be embedded in HTML output page)
    that will render at runtime a GoogleChart drawing inside a JavaScript-enabled browser of course.
    """

    def __init__(
        self,
        data=None,
        button_label="",
        combobox_label="",
        combobox_entry="",
        graph_source=GRAPH_SOURCE_DATA_BAREMETAL,
        graph_type=GRAPH_TYPE_AREA_CHART,
        graph_title="",
        stack_state=False,
        y_axes_titles=[],
        columns_for_2nd_yaxis=[],
        y_axes_max_value=None,
    ):
        self.data_table = data  # of type GoogleChartsGenericTable or GoogleChartsTimeSeries
        self.button_label = button_label
        self.combobox_label = combobox_label
        assert (len(self.button_label) == 0 and len(self.combobox_label) > 0) or (len(self.button_label) > 0 and len(self.combobox_label) == 0)
        self.combobox_entry = combobox_entry
        self.source_data = graph_source  # one of GRAPH_TYPE_BAREMETAL or GRAPH_TYPE_CGROUP
        self.graph_type = graph_type
        self.graph_title = graph_title
        self.stack_state = stack_state
        self.y_axes_titles = y_axes_titles
        self.columns_for_2nd_yaxis = columns_for_2nd_yaxis
        self.y_axes_max_value = y_axes_max_value
        self.graph_title += ", STACKED graph" if self.stack_state else ""

        # generate new JS name for this graph
        global g_num_generated_charts
        self.js_name = "graph" + str(g_num_generated_charts)
        g_num_generated_charts += 1

    def genGoogleChartJS_AreaChart(self):
        """After the JavaScript line graph data is output, the data is terminated and the graph options set"""
        global g_next_graph_need_stacking

        def __internalWriteAxis(series_indexes, target_axis_index):
            ret = ""
            for i, idx in enumerate(series_indexes, start=0):
                ret += "   %d: {targetAxisIndex:%d}" % (idx, target_axis_index)
                # print("i=%d, idx=%d, target_axis_index=%d" % (i,idx,target_axis_index))
                if i < len(series_indexes):
                    ret += ",\n"
                else:
                    ret += "\n"
            return ret

        ret_string = ""
        ret_string += "var options_%s = {\n" % (self.js_name)
        ret_string += '  chartArea: {left: "5%", width: "85%", top: "10%", height: "80%"},\n'
        ret_string += '  title: "%s",\n' % (self.graph_title)
        ret_string += '  focusTarget: "category",\n'

        # by default this tool plots the top 20 processes; in these cases both tooltips and legend will have up to 21 rows (including time)
        # so we make the font a bit smaller to make it more likely to view all the lines
        ret_string += "  tooltip: { textStyle: { fontSize: 12 } },\n"
        ret_string += "  legend: { textStyle: { fontSize: 12 } },\n"
        ret_string += '  explorer: { actions: ["dragToZoom", "rightClickToReset"], keepInBounds: true, maxZoomIn: 20.0 },\n'

        # HORIZONTAL AXIS
        ret_string += '  hAxis: { format: "%s", gridlines: { color: "lightgrey", count: 30 } },\n' % X_AXIS_DATEFORMAT

        # VERTICAL AXIS (OR AXES)
        if len(self.columns_for_2nd_yaxis) > 0:
            # compute indexes of series that use the 2nd Y axis:
            series_for_2nd_yaxis = [self.data_table.getDataSeriesIndexByName(colname) for colname in self.columns_for_2nd_yaxis]
            assert -1 not in series_for_2nd_yaxis
            # print("series_for_2nd_yaxis: %s" % ",".join(str(x) for x in series_for_2nd_yaxis))

            # compute indexes of series that use 1st Y axis:
            all_indexes = range(0, self.data_table.getNumDataSeries())
            series_for_1st_yaxis = [idx for idx in all_indexes if idx not in series_for_2nd_yaxis]
            # print("series_for_1st_yaxis: %s" % ",".join(str(x) for x in series_for_1st_yaxis))

            # assign data series to the 2 Y axes:
            ret_string += "  series: {\n"
            ret_string += __internalWriteAxis(series_for_1st_yaxis, 0)
            ret_string += __internalWriteAxis(series_for_2nd_yaxis, 1)
            ret_string += "  },\n"

            # allocate 2 Y axes:
            assert len(self.y_axes_titles) == 2
            if self.y_axes_max_value is None:
                ret_string += "  vAxes: {\n"
                ret_string += '    0: { title: "%s" },\n' % str(self.y_axes_titles[0])
                ret_string += '    1: { title: "%s" }\n' % str(self.y_axes_titles[1])
                ret_string += "  },\n"
            else:
                ret_string += "  vAxes: {\n"
                ret_string += '    0: { title: "%s", maxValue: %d },\n' % (str(self.y_axes_titles[0]), self.y_axes_max_value)
                ret_string += '    1: { title: "%s", maxValue: %d }\n' % (str(self.y_axes_titles[1]), self.y_axes_max_value)
                ret_string += "  },\n"
        else:
            # single vertical axis:
            assert len(self.y_axes_titles) == 1
            ret_string += '  vAxis: { title: "%s", gridlines: { color: "lightgrey", count: 11 } },\n' % str(self.y_axes_titles[0])

        # graph stacking
        g_next_graph_need_stacking = self.stack_state
        if g_next_graph_need_stacking:
            ret_string += "  isStacked:  1\n"
            g_next_graph_need_stacking = 0
        else:
            ret_string += "  isStacked:  0\n"

        ret_string += "};\n"  # end of "options_%s" variable
        ret_string += "\n"
        ret_string += "if (g_chart && g_chart.clearChart)\n"
        ret_string += "  g_chart.clearChart();\n"
        ret_string += 'g_chart = new google.visualization.AreaChart(document.getElementById("chart_master_div"));\n'
        ret_string += "g_chart.draw(data_%s, options_%s);\n" % (
            self.js_name,
            self.js_name,
        )
        ret_string += "g_current_data = data_%s;\n" % (self.js_name)
        ret_string += "g_current_options = options_%s;\n" % (self.js_name)
        return ret_string

    def genGoogleChartJS_BubbleChart(self):
        assert len(self.y_axes_titles) == 2
        ret_string = ""
        ret_string += "var options_%s = {\n" % (self.js_name)
        ret_string += '  explorer: { actions: ["dragToZoom", "rightClickToReset"], keepInBounds: true, maxZoomIn: 20.0 },\n'
        ret_string += '  chartArea: { left: "5%", width: "85%", top: "10%", height: "80%" },\n'
        ret_string += '  title: "%s",\n' % (self.graph_title)
        ret_string += '  hAxis: { title:"%s" },\n' % str(self.y_axes_titles[0])
        ret_string += '  vAxis: { title:"%s" },\n' % str(self.y_axes_titles[1])
        ret_string += "  sizeAxis: { maxSize: 200 },\n"
        ret_string += "  bubble: { textStyle: {fontSize: 15} }\n"
        ret_string += "};\n"  # end of "options_%s" variable
        ret_string += "\n"
        ret_string += "if (g_chart && g_chart.clearChart)\n"
        ret_string += "  g_chart.clearChart();\n"
        ret_string += 'g_chart = new google.visualization.BubbleChart(document.getElementById("chart_master_div"));\n'
        ret_string += "g_chart.draw(data_%s, options_%s);\n" % (
            self.js_name,
            self.js_name,
        )
        ret_string += "g_current_data = data_%s;\n" % (self.js_name)
        ret_string += "g_current_options = options_%s;\n" % (self.js_name)
        return ret_string

    def toGoogleChartJS(self):
        global g_next_graph_need_stacking

        # generate the JS
        js_code_inner = self.data_table.toGoogleChartTable(self.js_name)

        if self.graph_type == GRAPH_TYPE_AREA_CHART:
            js_code_inner += self.genGoogleChartJS_AreaChart()
        else:
            js_code_inner += self.genGoogleChartJS_BubbleChart()

        js_code = "function draw_%s() {\n" % (self.js_name)
        js_code += textwrap.indent(js_code_inner, " " * JS_INDENT_SIZE)

        # this graph will be activated by either
        #  - a button that should reset all comboboxes of the page
        #  - a combo box entry that should reset all other comboboxes in the page
        js_code += '  reset_combo_boxes("%s");\n' % self.combobox_label

        js_code += "}\n"  # end of draw_%s function
        js_code += "\n"

        return js_code


# =======================================================================================================
# HtmlOutputPage
# =======================================================================================================


class HtmlOutputPage:
    """
    This is able to produce a self-contained HTML page with embedded JavaScript to draw performance charts
    """

    def __init__(self, outfile, title):
        self.title = title
        self.outfile = outfile
        self.file = open(outfile, "w")  # Open the output file
        self.graphs = []

    def appendGoogleChart(self, chart):
        assert isinstance(chart, GoogleChartsGraph)
        self.graphs.append(chart)

    def startHtmlHead(self):
        """Write the head of the HTML webpage and start the JS section"""
        self.file.write(
            """<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <title>{pageTitle}</title>""".format(
                pageTitle=self.title
            )
        )

        self.file.write(
            """
  <style>
     html,body { height:85%; }
     body { background-color: #eaeaea; }
     h3 { margin: 0px; }
     ul { margin: 0 0 0 0;padding-left: 20px; }
     button { margin-bottom: 3px; }
     #monitored_system_span { background-color: white; color: red; padding: 4px; }
     #button_table { width:100%; border-collapse: collapse; }
     .button_table_col { border: darkgrey; border-style: solid; border-width: 2px; padding: 6px; margin: 6px; }
     #chart_master_div { width:98%; height:85%; border: darkgrey; border-style: solid; border-width: 2px; margin-left: auto; margin-right: auto}
     #chart_master_inner_div { position: absolute; top: 50%; left: 50%; -ms-transform: translate(-50%, -50%); transform: translate(-50%, -50%); }
     #chart_master_inner_p { font-size: x-large; }
     #bottom_div { float:left; max-width: 40%; border: darkgrey; border-style: solid; border-width: 2px; padding: 6px; margin: 6px; }
     #bottom_about_div { float:right; max-width: 15%; border: darkgrey; border-style: solid; border-width: 2px; padding: 6px; margin: 6px; }
     #bottom_div h3 { font-size: medium; }
     #bottom_div li { font-size: smaller; }
     #bottom_about_div h3 { font-size: medium; }
     #bottom_about_div li { font-size: smaller; }
     #bottom_table_val { font-family: monospace; }
  </style>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/pako@1.0.10/dist/pako.min.js"></script>
  <script type="text/javascript" src="https://www.gstatic.com/charts/loader.js"></script>
  <script type="text/javascript">
// Load the Visualization API and the corechart package; version 51 is June 2021 version
google.charts.load('51', {'packages':['corechart']});
google.setOnLoadCallback(setup_button_click_handlers);

/* The global chart object: */
var g_chart = null;

/* Currently selected data & options: */
var g_current_data = null;
var g_current_options = null;

/* The global window showing the configuration of all collected data: */
var g_configWindow = null;

/* Create a trigger for window resize that will redraw current chart */
var g_window_resize_timer = null;
window.addEventListener('resize', function(e){
  if (g_window_resize_timer) 
    clearTimeout(g_window_resize_timer);

  g_window_resize_timer = setTimeout(function() {
    if (g_current_data && g_current_options)
      g_chart.draw(g_current_data, g_current_options);
  }, 500);
});

/* Utility function used with combobox controls: */
function call_function_named(func_name) {
  eval(func_name + "()");
}

/* Utility function used to clear main graph: */
function clear_chart() {
  if (g_chart && g_chart.clearChart)
    g_chart.clearChart();
}

"""
        )
        # at this point we will generate all helper JS functions

    def endHtmlHead(self, config):
        """Finish the JS portion and HTML head tag"""

        # convert into JS all the charts that belong to this HTML document:
        combo_box_ctrls = set()
        for num, graph in enumerate(self.graphs, start=1):
            self.file.write(graph.toGoogleChartJS())
            if len(graph.combobox_label) > 0:
                combo_box_ctrls.add(graph.combobox_label)

        # add all event listeners for button clicks:
        self.file.write("function setup_button_click_handlers() {\n")
        for num, graph in enumerate(self.graphs, start=1):
            if len(graph.combobox_label) == 0:
                self.file.write('  document.getElementById("btn_draw_%s").addEventListener("click", draw_%s);\n' % (graph.js_name, graph.js_name))
            else:
                # this will be selected from a combobox, no need to hook into a button
                pass
        self.file.write('  document.getElementById("btn_show_config").addEventListener("click", show_config_window);\n')
        self.file.write("}\n")  # end of setup_button_click_handlers()

        # add function to reset all comboboxes
        self.file.write(
            """
/* Utility function used to reset combobox controls: */
function reset_combo_boxes(combobox_to_exclude_from_reset) {
"""
        )
        for num, comboname in enumerate(sorted(combo_box_ctrls), start=1):
            self.file.write('  if (combobox_to_exclude_from_reset != "%s")\n' % comboname)
            self.file.write('      document.getElementById("select_combobox_%s").value = "clear_chart";\n' % comboname)
        self.file.write("}\n")  # end of reset_combo_boxes()

        self.file.write(config)
        self.file.write("  </script>\n")
        self.file.write("</head>\n")

    def startHtmlBody(self, cgroupName, monitored_system):
        self.file.write("<body>\n")
        self.file.write('  <h1>Monitoring data collected from <span id="monitored_system_span">' + monitored_system + "</span></h1>\n")
        self.file.write('  <div id="button_div">\n')
        self.file.write('  <table id="button_table">\n')

        # Table header row
        self.file.write("  <tr>\n")
        self.file.write(
            '    <td class="button_table_col"></td><td class="button_table_col"><b>CGroup</b> stats (Data collected from %s)</td>\n' % cgroupName
        )
        self.file.write(
            '    <td class="button_table_col"></td><td class="button_table_col"><b>CGroup</b> per-process stats (Data collected from %s)</td>\n'
            % cgroupName
        )
        self.file.write('    <td class="button_table_col"><b>Baremetal</b> (Data collected from /proc)</td>\n')
        self.file.write("  </tr>\n")

        # Datarow
        self.file.write("  <tr>\n")
        self.file.write('  <td class="button_table_col">\n')
        self.file.write('    <button id="btn_show_config"><b>Configuration</b></button><br/>\n')
        self.file.write('  </td><td class="button_table_col">\n')

        def write_buttons_for_graph_type(source_data):
            nwritten_controls = 0

            # find all graphs that will be activated through a combobox
            graphs_combobox = {}
            for num, graph in enumerate(self.graphs, start=1):
                if graph.source_data == source_data and len(graph.combobox_label) > 0:
                    if graph.combobox_label not in graphs_combobox:
                        # add new dict entry as empty list
                        graphs_combobox[graph.combobox_label] = []

                    # add to the existing dict entry a new graph:
                    graphs_combobox[graph.combobox_label].append([graph.combobox_entry, graph.js_name])

            # generate the CPU select box:
            if len(graphs_combobox) > 0:
                for combobox_label in graphs_combobox.keys():
                    graph_list = graphs_combobox[combobox_label]
                    self.file.write('    <select id="select_combobox_%s" onchange="call_function_named(this.value)">\n' % (combobox_label))
                    self.file.write('      <option value="clear_chart">None</option>\n')
                    for entry in graph_list:
                        button_label = entry[0]
                        js_name = entry[1]
                        self.file.write('      <option value="draw_%s">%s</option>\n' % (js_name, button_label))
                    self.file.write("    </select>\n")
                    nwritten_controls += 1

            # find in all graphs registered so far all those related to the CGROUP
            for num, graph in enumerate(self.graphs, start=1):
                if graph.source_data == source_data:
                    if len(graph.combobox_label) > 0:
                        continue  # skip - already drawn via <select>
                    elif "CPU" in graph.button_label:
                        colour = "red"
                    elif graph.button_label.startswith("Memory"):
                        colour = "darkorange"
                    elif graph.button_label.startswith("Network"):
                        colour = "darkblue"
                    elif graph.button_label.startswith("Disk"):
                        colour = "darkgreen"
                    else:
                        colour = "black"
                    self.file.write(
                        '    <button id="btn_draw_' + graph.js_name + '" style="color:' + colour + '"><b>' + graph.button_label + "</b></button>\n"
                    )
                    nwritten_controls += 1

            if nwritten_controls == 0:
                self.file.write("N/A")

        write_buttons_for_graph_type(GRAPH_SOURCE_DATA_CGROUP)
        self.file.write('      </td><td class="button_table_col">\n')
        write_buttons_for_graph_type(GRAPH_SOURCE_DATA_BAREMETAL)

        self.file.write("  </td></tr>\n")
        self.file.write("  </table>\n")
        self.file.write("  </div>\n")
        self.file.write("  <p></p>\n")

        # finally generate the element where the main chart is going to be drawn:
        self.file.write(
            '  <div id="chart_master_div"><div id="chart_master_inner_div"><p id="chart_master_inner_p">...click on a button above to show a graph...</p></div></div>\n'
        )

    def appendHtmlTable(self, name, table_entries, div_id="bottom_div"):
        self.file.write("  <div id='" + div_id + "'>\n")
        self.file.write("    <h3>" + name + "</h3>\n")
        self.file.write("    <table>\n")
        self.file.write("    <tr><td><ul>\n")
        for i, entry in enumerate(table_entries, start=1):
            self.file.write("      <li>" + entry[0] + " <span id='bottom_table_val'>" + entry[1] + "</span></li>\n")
            if (i % 4) == 0:
                self.file.write("      </ul></td><td><ul>\n")
        self.file.write("    </ul></td></tr>\n")
        self.file.write("    </table>\n")
        self.file.write("  </div>\n")

    def endHtmlBody(self):
        self.file.write("</body>\n")
        self.file.write("</html>\n")
        self.file.close()


# =======================================================================================================
# CMonitorGraphGenerator
# =======================================================================================================


class CMonitorGraphGenerator:
    """
    This is the main class of cmonitor_chart, able to read a JSON file produced by cmonitor_collector,
    extract the most useful information and render them inside an HtmlOutputPage object.
    """

    def __init__(self, output_page, jheader, jdata):
        self.output_page = output_page  # must be of type HtmlOutputPage
        self.jheader = jheader  # a dictionary with cmonitor_collector "header" JSON object
        self.jdata = jdata  # a list of dictionaries with cmonitor_collector "samples" objects

        # in many places below we need to get "immutable" data that we know won't change across all samples
        # like the names of network devices or the list of CPUs...
        # since for some metrics the very sample does not contain any KPI (e.g. cgroup network traffic is generated
        # only for samples after the first one) if possible we pick the 2nd sample and not the 1st one:
        assert len(self.jdata) >= 2
        self.sample_template = self.jdata[1]

        # did we collect at PROCESS-level granularity or just at THREAD-level granularity?
        string_collected_kpis = self.jheader["cmonitor"]["collecting"]  # e.g. "cgroup_cpu,cgroup_memory,cgroup_threads"
        self.collected_threads = "cgroup_threads" in string_collected_kpis
        if verbose:
            print("Threads (instead of processes) have been collected in the input JSON file.")

        # detect num of CPUs:
        self.baremetal_logical_cpus_indexes = []
        if "stat" in self.sample_template:
            self.baremetal_logical_cpus_indexes = CMonitorGraphGenerator.collect_logical_cpu_indexes_from_section(self.sample_template, "stat")
            if verbose:
                print(
                    "Found %d CPUs in baremetal stats with logical indexes [%s]"
                    % (
                        len(self.baremetal_logical_cpus_indexes),
                        ", ".join(str(x) for x in self.baremetal_logical_cpus_indexes),
                    )
                )

        self.cgroup_logical_cpus_indexes = []
        if "cgroup_cpuacct_stats" in self.sample_template:
            self.cgroup_logical_cpus_indexes = CMonitorGraphGenerator.collect_logical_cpu_indexes_from_section(
                self.sample_template, "cgroup_cpuacct_stats"
            )
            if verbose:
                print(
                    "Found %d CPUs in cgroup stats with logical indexes [%s]"
                    % (
                        len(self.cgroup_logical_cpus_indexes),
                        ", ".join(str(x) for x in self.cgroup_logical_cpus_indexes),
                    )
                )

    # =======================================================================================================
    # Private helpers
    # =======================================================================================================

    @staticmethod
    def collect_logical_cpu_indexes_from_section(jsample, section_name):
        """
        Walks over given JSON sample looking for keys 'cpuXYZ' and storing all 'XYZ' CPU indexes.
        Returns a list of CPU indexes
        """
        logical_cpus_indexes = []
        for key in jsample[section_name]:
            if key.startswith("cpu") and key != "cpu_total" and key != "cpu_tot":
                cpuIdx = int(key[3:])
                logical_cpus_indexes.append(cpuIdx)
                # print("%s %s" %(key, cpuIdx))
        return logical_cpus_indexes

    def __choose_byte_divider(self, mem_total_bytes):
        divider = 1
        unit = "Bytes"
        if mem_total_bytes > 9e9:
            divider = 1e9
            unit = "GB"
        elif mem_total_bytes > 9e6:
            divider = 1e6
            unit = "MB"
        # print("%d -> %s, %d" % (mem_total_bytes,unit, divider))
        return (divider, unit)

    def __print_data_loading_stats(self, desc, n_invalid_samples):
        if n_invalid_samples > 0:
            print(
                "While parsing %s statistics found %d/%d (%.1f%%) samples that did not contain some required JSON section."
                % (
                    desc,
                    n_invalid_samples,
                    len(self.jdata),
                    100 * n_invalid_samples / len(self.jdata),
                )
            )
        else:
            print("Parsed correctly %d samples for [%s] category" % (len(self.jdata), desc))

    def __cgroup_get_cpu_quota_percentage(self):
        """
        Returns a number, in percentage, that indicates how much&many CPUs can be used.
        E.g. possible values are 50%, 140%, 300% or -1 to indicate no limit.
        """
        cpu_quota_perc = 100
        if "cpu_quota_perc" in self.jheader["cgroup_config"]:
            cpu_quota_perc = 100 * self.jheader["cgroup_config"]["cpu_quota_perc"]
            if cpu_quota_perc == -100:  # means there's no CPU limit
                cpu_quota_perc = -1
        return cpu_quota_perc

    @staticmethod
    def cgroup_get_cpu_throttling(s):
        cpu_throttling = 0
        if "throttling" in s["cgroup_cpuacct_stats"]:
            # throttling is new since cmonitor_collector 1.5-0
            nr_periods = s["cgroup_cpuacct_stats"]["throttling"]["nr_periods"]
            if nr_periods:
                cpu_throttling = 100 * s["cgroup_cpuacct_stats"]["throttling"]["nr_throttled"] / nr_periods
        return cpu_throttling

    def __cgroup_get_memory_limit(self):
        if "memory_limit_bytes" in self.jheader["cgroup_config"]:
            # IMPORTANT: this value could be -1 if there's no limit
            cgroup_limit_bytes = self.jheader["cgroup_config"]["memory_limit_bytes"]

        if self.jheader["cgroup_config"]["version"] == 1:
            # cgroups v1
            system_total_bytes = self.sample_template["cgroup_memory_stats"]["stat.cache"] + self.sample_template["cgroup_memory_stats"]["stat.rss"]
        else:
            # cgroups v2
            if "proc_meminfo" in self.jheader:
                system_total_bytes = self.jheader["proc_meminfo"]["MemTotal"]
            else:
                # we have no clue what will be the "max memory" to plot (unless we do full scanning of all samples,
                # which of course is very slow)... so we assume that twice of memory used by first sample will be
                # a good approx:
                system_total_bytes = self.sample_template["cgroup_memory_stats"]["stat.current"] * 2

        if cgroup_limit_bytes == -1:
            divider, unit = self.__choose_byte_divider(system_total_bytes)
        else:
            divider, unit = self.__choose_byte_divider(cgroup_limit_bytes)
        return cgroup_limit_bytes, divider, unit

    # =======================================================================================================
    # Public API
    # =======================================================================================================

    def generate_config_viewer(self):
        """
        Creates into output document a snippet of Javascript code to open a new HTML window with
        a basic rendering of cmonitor_collector configuration and the monitored server.
        """

        # ----- add config box
        def configdump(section, displayName):
            # newstr = '<h3>' + displayName + '</h3>\\\n'
            newstr = "<tr><td colspan='2' id='sectioncol'>" + displayName + "</td></tr>\\\n"
            config_dict = self.jheader[section]
            for label in config_dict:
                newstr += "    <tr>\\\n"
                newstr += "    <td id='configkey'>%s</td><td id='configval'>%s</td>\\\n" % (
                    label.capitalize().replace("_", " "),
                    str(config_dict[label]),
                )
                newstr += "    </tr>\\\n"
            return newstr

        def sizeof_fmt(num, suffix="B"):
            for unit in ["", "Ki", "Mi", "Gi", "Ti", "Pi", "Ei", "Zi"]:
                if abs(num) < 1024.0:
                    return "%3.1f%s%s" % (num, unit, suffix)
                num /= 1024.0
            return "%.1f%s%s" % (num, "Yi", suffix)

        # provide some human-readable config files:
        if "cgroup_config" in self.jheader:
            avail_cpus = self.jheader["cgroup_config"]["cpus"].split(",")
            self.jheader["cgroup_config"]["num_allowed_cpus"] = len(avail_cpus)
            self.jheader["cgroup_config"]["memory_limit_bytes"] = sizeof_fmt(int(self.jheader["cgroup_config"]["memory_limit_bytes"]))
            self.jheader["cgroup_config"]["cpus"] = self.jheader["cgroup_config"]["cpus"].replace(",", ", ")

        if "cmonitor" in self.jheader:
            if self.jheader["cmonitor"]["sample_num"] == 0:
                self.jheader["cmonitor"]["sample_num"] = "Infinite"

        if "proc_meminfo" in self.jheader:
            self.jheader["proc_meminfo"]["MemTotal"] = sizeof_fmt(int(self.jheader["proc_meminfo"]["MemTotal"]))
            self.jheader["proc_meminfo"]["Hugepagesize"] = sizeof_fmt(int(self.jheader["proc_meminfo"]["Hugepagesize"]))

        config_str = ""
        config_str += "\nfunction show_config_window() {\n"
        config_str += "    if (g_configWindow) g_configWindow.close();\n"
        config_str += '    g_configWindow = window.open("", "MsgWindow", "width=1024, height=800, toolbar=no");\n'
        config_str += '    g_configWindow.document.write("\\\n'
        config_str += "    <html><head>\\\n"
        config_str += "      <title>Configuration</title>\\\n"
        config_str += "      <style>\\\n"
        config_str += "        table { padding-left: 2ex; }\\\n"
        config_str += "        #sectioncol {font-weight: bold; padding: 1ex; font-size: large;background-color: lightsteelblue;}\\\n"
        config_str += "        #configkey {font-weight: bold;}\\\n"
        config_str += "        #configval {font-family: monospace;}\\\n"
        config_str += "      </style>\\\n"
        config_str += "    </head><body>\\\n"
        config_str += "      <h2>Monitored System Summary</h2>\\\n"
        config_str += "      <table>\\\n"
        config_str += configdump("identity", "Server Identity")
        config_str += configdump("os_release", "Operating System Release")
        config_str += configdump("proc_version", "Linux Kernel Version")
        if "cgroup_config" in self.jheader:  # if cgroups are off, this section will not be present
            config_str += configdump("cgroup_config", "Linux Control Group (CGroup) Configuration")
        if "lscpu" in self.jheader:  # Alpine docker has no lscpu utility
            config_str += configdump("lscpu", "CPU Overview")
        if "proc_meminfo" in self.jheader:
            config_str += configdump("proc_meminfo", "Memory Overview")
        # config_str += configdump("cpuinfo", "CPU Core Details")
        config_str += "      </table>\\\n"
        config_str += "      <h2>Monitoring Summary</h2>\\\n"
        config_str += "      <table>\\\n"
        config_str += configdump("cmonitor", "Performance Stats Collector Configuration")
        config_str += "      </table>\\\n"
        config_str += "    </body></html>\\\n"
        config_str += '");\n}\n\n'

        self.output_page.endHtmlHead(config_str)

    def __generate_topN_procs_bubble_chart(self, process_dict, topN_process_pids_list, max_byte_value_dict, chart_desc_prefix):
        mem_divider, mem_unit = self.__choose_byte_divider(max_byte_value_dict["mem_rss"])
        io_divider, io_unit = self.__choose_byte_divider(max_byte_value_dict["io_total"])
        cpu_label = "CPU time"
        io_label = "I/O " + io_unit
        thread_proc_label = "Thread" if self.collected_threads else "Process"
        memory_label = "Memory " + mem_unit

        def get_nice_process_or_thread_name(pid):
            return "%s (%d)" % (process_dict[pid]["cmd"], pid)

        # now select the N top processes and put their data in a GoogleChart table:
        topN_process_table = GoogleChartsGenericTable(["Command", cpu_label, io_label, thread_proc_label, memory_label])
        for i, pid in enumerate(topN_process_pids_list):
            p = process_dict[pid]
            nicecmd = get_nice_process_or_thread_name(pid)
            if verbose:
                print("Processing data for %d-th CPU-top-scorer process [%s]" % (i + 1, nicecmd))
            topN_process_table.addRow([p["cmd"], p["cpu"], p["io"] / io_divider, nicecmd, p["mem"] / mem_divider])

        # generate the bubble chart graph:
        self.output_page.appendGoogleChart(
            GoogleChartsGraph(
                data=topN_process_table,
                button_label="CPU/Memory/Disk Bubbles by Thread" if self.collected_threads else "CPU/Memory/Disk Bubbles by Process",
                graph_source=GRAPH_SOURCE_DATA_CGROUP,
                graph_type=GRAPH_TYPE_BUBBLE_CHART,
                graph_title=chart_desc_prefix + "for CPU/disk total usage on X/Y axes; memory usage as bubble size (from cgroup stats)",
                y_axes_titles=[cpu_label, io_label],
            )
        )
        if verbose:
            print("Generated bubble chart successfully.")

    def __generate_topN_procs_cpu_io_mem_vs_time(self, process_dict, topN_process_pids_list, chart_desc_prefix):
        # now that first pass is done, adjust units for memory
        mem_limit_bytes, mem_divider, mem_unit = self.__cgroup_get_memory_limit()
        cpu_quota_perc = self.__cgroup_get_cpu_quota_percentage()

        def get_nice_process_or_thread_name(pid):
            return "%s (%d)" % (process_dict[pid]["cmd"], pid)

        # now generate instead a table of CPU/IO/MEMORY usage over time per process:
        process_table = {}

        if cpu_quota_perc > 0:
            # it is possible to compute the "idle" time inside this cgroup and it's possible that CPU is throttled...
            # so in such case we add 2 more data series to the chart:
            process_table["cpu"] = GoogleChartsTimeSeries(
                ["Timestamp"] + [get_nice_process_or_thread_name(pid) for pid in topN_process_pids_list] + ["Idle", "Throttling"]
            )
            y_axes_max_value = cpu_quota_perc * 1.1
            columns_for_2nd_yaxis = ["Throttling"]
            y_axes_titles = ["CPU (%)", "CPU Throttling (%)"]
        else:
            # no CPU limit... creating an "idle" column (considering as max CPU all the CPUs available) tend to produce weird
            # results out-of-scale (imagine servers with hundreds of CPUs and cmonitor_collector monitoring just a Redis container!)
            # so we do not place any "idle" column. The "throttling" column does not apply either.
            assert cpu_quota_perc == -1
            process_table["cpu"] = GoogleChartsTimeSeries(["Timestamp"] + [get_nice_process_or_thread_name(pid) for pid in topN_process_pids_list])
            y_axes_max_value = None  # let Google Charts autocompute the Y axes limits
            columns_for_2nd_yaxis = []
            y_axes_titles = ["CPU (%)"]

        process_table["io"] = GoogleChartsTimeSeries(["Timestamp"] + [get_nice_process_or_thread_name(pid) for pid in topN_process_pids_list])
        process_table["mem"] = GoogleChartsTimeSeries(
            ["Timestamp", "Limit", "Alloc Failures"] + [get_nice_process_or_thread_name(pid) for pid in topN_process_pids_list]
        )

        max_byte_value_dict = {}
        max_byte_value_dict["mem_rss"] = 0
        max_byte_value_dict["io_total"] = 0
        for sample in self.jdata:
            try:
                row = {}
                for key in ["cpu", "io", "mem"]:
                    row[key] = [sample["timestamp"]["UTC"]]

                # Memory graph has
                # - limit
                # - alloc failures
                # as additional columns right after the timestamp
                row["mem"].append(mem_limit_bytes / mem_divider)
                row["mem"].append(sample["cgroup_memory_stats"]["failcnt"])

                tot_cpu_usage_perc = 0
                for top_process_pid in topN_process_pids_list:
                    # print(top_process_pid)
                    json_key = "pid_%s" % top_process_pid
                    if json_key in sample["cgroup_tasks"]:
                        top_proc_sample = sample["cgroup_tasks"][json_key]

                        cpu = top_proc_sample["cpu_tot"]
                        io = top_proc_sample["io_rchar"] + top_proc_sample["io_wchar"]
                        mem = top_proc_sample["mem_rss_bytes"] / mem_divider

                        tot_cpu_usage_perc += cpu
                        row["cpu"].append(cpu)
                        row["io"].append(io)
                        row["mem"].append(mem)

                        # keep track of maxs:
                        max_byte_value_dict["mem_rss"] = max(mem, max_byte_value_dict["mem_rss"])
                        max_byte_value_dict["io_total"] = max(io, max_byte_value_dict["io_total"])
                    else:
                        # probably this process was born later or dead earlier than this timestamp
                        row["cpu"].append(0)
                        row["io"].append(0)
                        row["mem"].append(0)

                # CPU graph has
                # - idle (if cpu_quota_perc > 0)
                # - throttling
                # as additional columns right after the last PID serie
                if cpu_quota_perc > 0:
                    row["cpu"].append(max(cpu_quota_perc - tot_cpu_usage_perc, 0))
                    row["cpu"].append(CMonitorGraphGenerator.cgroup_get_cpu_throttling(sample))

                for key in ["cpu", "io", "mem"]:
                    process_table[key].addRow(row[key])
            except KeyError:  # avoid crashing if a key is not present in the dictionary...
                # print("Missing cgroup data while parsing sample %d" % i)
                pass

        # CPU by thread/process:
        # take any contribute of any thread/process and stack it together: this way it becomes easier to spot
        # when the cgroup CPU limit was hit and due to which threads/processes
        self.output_page.appendGoogleChart(
            GoogleChartsGraph(
                data=process_table["cpu"],
                graph_title=chart_desc_prefix + "for CPU usage (from cgroup stats)",
                button_label="CPU by Thread" if self.collected_threads else "CPU by Process",
                graph_source=GRAPH_SOURCE_DATA_CGROUP,
                stack_state=True,
                y_axes_titles=y_axes_titles,
                # throttling should not be stacked to the CPU usage contributions, so move it on 2nd y axis:
                columns_for_2nd_yaxis=columns_for_2nd_yaxis,
                # make the 2 axes have the same identical Y scale to make it easier to read it:
                y_axes_max_value=y_axes_max_value,
            )
        )

        self.output_page.appendGoogleChart(
            GoogleChartsGraph(
                data=process_table["io"],
                graph_title=chart_desc_prefix + "for I/O usage (from cgroup stats)",
                button_label="IO by Thread" if self.collected_threads else "IO by Process",
                y_axes_titles=["IO Read+Write (Bytes Per Sec)"],
                graph_source=GRAPH_SOURCE_DATA_CGROUP,
                stack_state=False,
            )
        )

        # FIXME FIXME: how to remove thread contributes and keep only main-thread contributes here for memory graph???
        # NOTE: "memory by thread" does not make much sense since Linux does not keep different memory space for different
        #       threads that belong to the same process; for this reason we produce always the "memory by process" graph:
        self.output_page.appendGoogleChart(
            GoogleChartsGraph(
                data=process_table["mem"],
                graph_title=chart_desc_prefix + "for memory usage (from cgroup stats)",
                button_label="Memory by Thread" if self.collected_threads else "Memory by Process",
                y_axes_titles=["RSS Memory (%s)" % mem_unit],
                graph_source=GRAPH_SOURCE_DATA_CGROUP,
                stack_state=False,
            )
        )
        if verbose:
            print("Generated per-process/per-thread CPU, IO and MEMORY charts successfully.")

    def generate_cgroup_topN_procs(self, numProcsToShow=20, thread_filter=""):
        # if process data was not collected, just return:
        if "cgroup_tasks" not in self.sample_template:
            return

        # build a dictionary containing cumulative metrics for CPU/IO/MEM data for each process
        # along all collected samples
        process_dict = {}
        max_byte_value_dict = {}
        max_byte_value_dict["mem_rss"] = 0
        max_byte_value_dict["io_total"] = 0
        n_invalid_samples = 0
        for i, sample in enumerate(self.jdata):
            try:
                for process in sample["cgroup_tasks"]:
                    # parse data from JSON
                    entry = sample["cgroup_tasks"][process]
                    cmd = entry["cmd"]
                    # filter by thread name if there is a filter
                    if thread_filter and thread_filter not in cmd:
                        continue
                    cputime = entry["cpu_usr_total_secs"] + entry["cpu_sys_total_secs"]
                    iobytes = entry["io_total_read"] + entry["io_total_write"]
                    membytes = entry["mem_rss_bytes"]  # take RSS, more realistic/useful compared to the "mem_virtual_bytes"
                    thepid = entry["pid"]  # can be the TID (thread ID) if cmonitor_collector was started with --collect=cgroup_threads

                    # keep track of maxs:
                    max_byte_value_dict["mem_rss"] = max(membytes, max_byte_value_dict["mem_rss"])
                    max_byte_value_dict["io_total"] = max(iobytes, max_byte_value_dict["io_total"])

                    try:  # update the current entry
                        process_dict[thepid]["cpu"] = cputime
                        process_dict[thepid]["io"] = iobytes
                        process_dict[thepid]["mem"] = membytes
                        process_dict[thepid]["cmd"] = cmd

                        # FIXME FIXME
                        # process_dict[thepid]["is_thread"] =
                    except:  # no current entry so add one
                        process_dict.update(
                            {
                                thepid: {
                                    "cpu": cputime,
                                    "io": iobytes,
                                    "mem": membytes,
                                    "cmd": cmd,
                                }
                            }
                        )
            except KeyError as e:  # avoid crashing if a key is not present in the dictionary...
                print(f"Missing cgroup data while parsing {i}-th sample: {e}")
                n_invalid_samples += 1
                pass

        self.__print_data_loading_stats("per-process", n_invalid_samples)

        # now sort all collected processes by the amount of CPU*memory used:
        # NOTE: sorted() will return just the sorted list of KEYs = PIDs
        def sort_key(d):
            # return process_dict[d]['cpu'] * process_dict[d]['mem']
            return process_dict[d]["cpu"]

        topN_process_pids_list = sorted(process_dict, key=sort_key, reverse=True)

        # truncate to first N:
        if numProcsToShow > 0:
            topN_process_pids_list = topN_process_pids_list[0:numProcsToShow]

        # did we collect processes or threads?
        if self.collected_threads:
            chart_desc_prefix = "Top %d threads " % numProcsToShow
        else:
            chart_desc_prefix = "Top %d processes " % numProcsToShow

        self.__generate_topN_procs_bubble_chart(process_dict, topN_process_pids_list, max_byte_value_dict, chart_desc_prefix)
        self.__generate_topN_procs_cpu_io_mem_vs_time(process_dict, topN_process_pids_list, chart_desc_prefix)

    def generate_baremetal_disks_io(self):
        # if disk data was not collected, just return:
        if "disks" not in self.sample_template:
            return

        all_disks = self.sample_template["disks"].keys()
        if len(all_disks) == 0:
            return

        # see https://www.kernel.org/doc/Documentation/iostats.txt

        diskcols = ["Timestamp"]
        for device in all_disks:
            # diskcols.append(str(device) + " Disk Time")
            # diskcols.append(str(device) + " Reads")
            # diskcols.append(str(device) + " Writes")
            diskcols.append(str(device) + " Read MB")
            diskcols.append(str(device) + " Write MB")

        # convert from kB to MB
        divider = 1000

        #
        # MAIN LOOP
        # Process JSON sample and fill the GoogleChartsTimeSeries() object
        #

        disk_table = GoogleChartsTimeSeries(diskcols)
        for i, s in enumerate(self.jdata):
            if i == 0:
                continue

            row = []
            row.append(s["timestamp"]["UTC"])
            for device in all_disks:
                # row.append(s["disks"][device]["time"])
                # row.append(s["disks"][device]["reads"])
                # row.append(s["disks"][device]["writes"])
                row.append(s["disks"][device]["rkb"] / divider)
                row.append(-s["disks"][device]["wkb"] / divider)
            disk_table.addRow(row)

        self.output_page.appendGoogleChart(
            GoogleChartsGraph(
                data=disk_table,
                button_label="Disk I/O",
                graph_source=GRAPH_SOURCE_DATA_BAREMETAL,
                graph_title="Disk I/O (from baremetal stats)",
                y_axes_titles=["MB"],
            )
        )
        return

    # def generate_filesystems(self.output_page, self.jdata):
    #     global self.graphs
    #     fsstr = ""
    #     for fs in self.sample_template["filesystems"].keys():
    #         fsstr = fsstr + "'" + fs + "',"
    #     fsstr = fsstr[:-1]
    #     startHtmlHead_line_graph(self.output_page, fsstr)
    #     for i, s in enumerate(self.jdata):
    #         self.output_page.write(",['Date(%s)' " % (googledate(s['timestamp']["UTC"])))
    #         for fs in s["filesystems"].keys():
    #             self.output_page.write(", %.1f" % (s["filesystems"][fs]["fs_full_percent"]))
    #         self.output_page.write("]\n")
    #     self.output_page.appendGoogleChart(GoogleChartsGraph( 'File Systems Used percent')
    #     return

    def __generate_network_traffic_graphs(self, graph_source, sample_section_name, graph_desc):
        # if network traffic data was not collected, just return:
        if sample_section_name not in self.sample_template:
            return

        all_netdevices = self.sample_template[sample_section_name].keys()
        if len(all_netdevices) == 0:
            return

        netcols = ["Timestamp"]
        for device in all_netdevices:
            netcols.append(str(device) + "+in")
            netcols.append(str(device) + "-out")

        # convert from bytes to MB
        divider = 1000 * 1000

        #
        # MAIN LOOP
        # Process JSON sample and fill the GoogleChartsTimeSeries() object
        #

        # MB/sec

        net_table = GoogleChartsTimeSeries(netcols)
        for i, s in enumerate(self.jdata):
            if i == 0:
                continue

            row = [s["timestamp"]["UTC"]]
            for device in all_netdevices:
                try:
                    row.append(+s[sample_section_name][device]["ibytes"] / divider)
                    row.append(-s[sample_section_name][device]["obytes"] / divider)
                except KeyError:
                    if verbose:
                        print("Missing key '%s' while parsing sample %d" % (device, i))
                    row.append(0)
                    row.append(0)
            net_table.addRow(row)

        self.output_page.appendGoogleChart(
            GoogleChartsGraph(
                data=net_table,
                graph_title=f"Network Traffic in MB/s {graph_desc}",
                button_label="Network Traffic (MB/s)",
                y_axes_titles=["MB/s"],
                graph_source=graph_source,
                stack_state=False,
            )
        )

        # PPS

        net_table = GoogleChartsTimeSeries(netcols)
        for i, s in enumerate(self.jdata):
            if i == 0:
                continue

            row = [s["timestamp"]["UTC"]]
            for device in all_netdevices:
                try:
                    row.append(+s[sample_section_name][device]["ipackets"])
                    row.append(-s[sample_section_name][device]["opackets"])
                except KeyError:
                    if verbose:
                        print("Missing key '%s' while parsing sample %d" % (device, i))
                    row.append(0)
                    row.append(0)
            net_table.addRow(row)

        self.output_page.appendGoogleChart(
            GoogleChartsGraph(
                data=net_table,
                graph_title=f"Network Traffic in PPS {graph_desc}",
                button_label="Network Traffic (PPS)",
                y_axes_titles=["PPS"],
                graph_source=graph_source,
                stack_state=False,
            )
        )
        return

    def generate_baremetal_network_traffic(self):
        self.__generate_network_traffic_graphs(GRAPH_SOURCE_DATA_BAREMETAL, "network_interfaces", "(from baremetal stats)")

    def generate_cgroup_network_traffic(self):
        self.__generate_network_traffic_graphs(GRAPH_SOURCE_DATA_CGROUP, "cgroup_network", "(from cgroup stats)")

    def generate_baremetal_cpus(self):
        # if baremetal CPU data was not collected, just return:
        if "stat" not in self.sample_template:
            return

        # prepare empty tables
        baremetal_cpu_stats = {}
        for c in self.baremetal_logical_cpus_indexes:
            baremetal_cpu_stats[c] = GoogleChartsTimeSeries(
                [
                    "Timestamp",
                    "User",
                    "Nice",
                    "System",
                    "Idle",
                    "I/O wait",
                    "Hard IRQ",
                    "Soft IRQ",
                    "Steal",
                ]
            )

        all_cpus_table = GoogleChartsTimeSeries(["Timestamp"] + [("CPU" + str(x)) for x in self.baremetal_logical_cpus_indexes])

        #
        # MAIN LOOP
        # Process JSON sample and fill the GoogleChartsTimeSeries() object
        #

        for i, s in enumerate(self.jdata):
            if i == 0:
                continue  # skip first sample

            ts = s["timestamp"]["UTC"]
            all_cpus_row = [ts]
            for c in self.baremetal_logical_cpus_indexes:
                cpu_stats = s["stat"]["cpu" + str(c)]
                cpu_total = (
                    cpu_stats["user"]
                    + cpu_stats["nice"]
                    + cpu_stats["sys"]
                    + cpu_stats["iowait"]
                    + cpu_stats["hardirq"]
                    + cpu_stats["softirq"]
                    + cpu_stats["steal"]
                )
                baremetal_cpu_stats[c].addRow(
                    [
                        ts,
                        cpu_stats["user"],
                        cpu_stats["nice"],
                        cpu_stats["sys"],
                        cpu_stats["idle"],
                        cpu_stats["iowait"],
                        cpu_stats["hardirq"],
                        cpu_stats["softirq"],
                        cpu_stats["steal"],
                    ]
                )
                all_cpus_row.append(cpu_total)

            all_cpus_table.addRow(all_cpus_row)

        # Produce the javascript:
        for c in self.baremetal_logical_cpus_indexes:
            self.output_page.appendGoogleChart(
                GoogleChartsGraph(
                    data=baremetal_cpu_stats[c],  # Data
                    graph_title="Logical CPU " + str(c) + " (from baremetal stats)",
                    combobox_label="baremetal_cpus",
                    combobox_entry="CPU" + str(c),
                    y_axes_titles=["Time (%)"],
                    graph_source=GRAPH_SOURCE_DATA_BAREMETAL,
                    stack_state=True,
                )
            )

        # Also produce the "all CPUs" graph
        self.output_page.appendGoogleChart(
            GoogleChartsGraph(
                data=all_cpus_table,  # Data
                graph_title="All logical CPUs (from baremetal stats)",
                button_label="All CPUs",
                y_axes_titles=["Time (%)"],
                graph_source=GRAPH_SOURCE_DATA_BAREMETAL,
                stack_state=False,
            )
        )
        return

    def generate_cgroup_cpus(self):
        if "cgroup_cpuacct_stats" not in self.sample_template:
            return  # cgroup mode not enabled at collection time!

        # prepare empty tables
        cpu_stats_table = {}
        for c in self.cgroup_logical_cpus_indexes:
            cpu_stats_table[c] = GoogleChartsTimeSeries(["Timestamp", "User", "System"])

        all_cpus_table = GoogleChartsTimeSeries(
            ["Timestamp", "Limit/Quota", "Throttling"] + [("CPU" + str(x)) for x in self.cgroup_logical_cpus_indexes]
        )

        #
        # MAIN LOOP
        # Process JSON sample and fill the GoogleChartsTimeSeries() object
        #

        cpu_quota_perc = self.__cgroup_get_cpu_quota_percentage()
        n_invalid_samples = 0
        for i, s in enumerate(self.jdata):
            if i == 0:
                continue  # skip first sample

            try:
                ts = s["timestamp"]["UTC"]

                all_cpus_row = [ts, cpu_quota_perc, CMonitorGraphGenerator.cgroup_get_cpu_throttling(s)]
                for c in self.cgroup_logical_cpus_indexes:
                    # get data:
                    cpu_stats = s["cgroup_cpuacct_stats"]["cpu" + str(c)]
                    if "sys" in cpu_stats:
                        cpu_sys = cpu_stats["sys"]
                    else:
                        cpu_sys = 0
                    cpu_total = cpu_stats["user"] + cpu_sys

                    # append data:
                    cpu_stats_table[c].addRow([ts, cpu_stats["user"], cpu_sys])
                    all_cpus_row.append(cpu_total)

                all_cpus_table.addRow(all_cpus_row)
            except KeyError:  # avoid crashing if a key is not present in the dictionary...
                # print("Missing cgroup data while parsing sample %d" % i)
                n_invalid_samples += 1
                pass

        self.__print_data_loading_stats("cgroup CPU", n_invalid_samples)

        # Produce 1 graph for each CPU:
        for c in self.cgroup_logical_cpus_indexes:
            self.output_page.appendGoogleChart(
                GoogleChartsGraph(
                    data=cpu_stats_table[c],  # Data
                    graph_title="Logical CPU " + str(c) + " (from CGroup stats)",
                    combobox_label="cgroup_cpus",
                    combobox_entry="CPU" + str(c),
                    y_axes_titles=["Time (%)"],
                    graph_source=GRAPH_SOURCE_DATA_CGROUP,
                    stack_state=True,
                )
            )

        ver = int(self.jheader["cgroup_config"]["version"])

        # Also produce the "all CPUs" graph that includes some very useful KPIs like
        # - CPU limit imposed on Linux CFS scheduler
        # - Amount of CPU throttling
        # NOTE: when cgroups v2 are used, there's no per-CPU stat just the total CPU usage,
        #       so we change the title of the tab to reflect that
        self.output_page.appendGoogleChart(
            GoogleChartsGraph(
                data=all_cpus_table,  # Data
                graph_title="All logical CPUs available in the cgroup" if ver == 1 else "CPU usage measured in the cgroup",
                button_label="All CPUs" if ver == 1 else "CPU",
                y_axes_titles=["Time (%)"],
                graph_source=GRAPH_SOURCE_DATA_CGROUP,
                stack_state=False,
            )
        )

        return

    def generate_baremetal_memory(self):
        # if baremetal memory data was not collected, just return:
        if "proc_meminfo" not in self.sample_template:
            return

        #
        # MAIN LOOP
        # Process JSON sample and build Google Chart-compatible Javascript variable
        # See https://developers.google.com/chart/interactive/docs/reference
        #

        mem_total_bytes = self.sample_template["proc_meminfo"]["MemTotal"]
        baremetal_memory_stats = GoogleChartsTimeSeries(["Timestamp", "Used", "Cached (DiskRead)", "Free"])
        divider, unit = self.__choose_byte_divider(mem_total_bytes)

        for i, s in enumerate(self.jdata):
            if i == 0:
                continue  # skip first sample
            meminfo_stats = s["proc_meminfo"]

            if meminfo_stats["MemTotal"] != mem_total_bytes:
                continue  # this is impossible AFAIK (hot swap of memory is not handled!!)

            #
            # NOTE: most tools like e.g. free -k just map:
            #
            #   free output |   corresponding /proc/meminfo fields
            # --------------+---------------------------------------
            #   Mem: total  |   MemTotal
            #   Mem: used   |   MemTotal - MemFree - Buffers - Cached - Slab
            #   Mem: free   |   MemFree             ^^^^^^^^^           ^^^^
            #                                        Buffers and Slab are close to zero 99% of the time
            #
            # see https://access.redhat.com/solutions/406773

            mf = meminfo_stats["MemFree"]
            mc = meminfo_stats["Cached"]

            baremetal_memory_stats.addRow(
                [
                    s["timestamp"]["UTC"],
                    int((mem_total_bytes - mf - mc) / divider),  # compute used memory
                    int(mc / divider),  # cached
                    int(mf / divider),  # free
                ]
            )

        # Produce the javascript:
        self.output_page.appendGoogleChart(
            GoogleChartsGraph(
                data=baremetal_memory_stats,  # Data
                graph_title="Memory usage in " + unit + " (from baremetal stats)",
                button_label="Memory",
                y_axes_titles=[unit],
                graph_source=GRAPH_SOURCE_DATA_BAREMETAL,
                stack_state=True,
            )
        )
        return

    def generate_cgroup_memory(self):
        # if cgroup data was not collected, just return:
        if "cgroup_memory_stats" not in self.sample_template:
            return

        #
        # MAIN LOOP
        # Process JSON sample and build Google Chart-compatible Javascript variable
        # See https://developers.google.com/chart/interactive/docs/reference
        #

        cgroup_memory_stats = GoogleChartsTimeSeries(["Timestamp", "Used", "Cached (DiskRead)", "Alloc Failures", "Limit"])

        ver = int(self.jheader["cgroup_config"]["version"])

        mem_limit_bytes, divider, unit = self.__cgroup_get_memory_limit()

        n_invalid_samples = 0
        for i, s in enumerate(self.jdata):
            if i == 0:
                continue  # skip first sample

            try:
                if ver == 1:
                    mu = s["cgroup_memory_stats"]["stat.rss"]
                    mc = s["cgroup_memory_stats"]["stat.cache"]
                    mfail = s["cgroup_memory_stats"]["failcnt"]
                else:
                    # cgroups v2
                    mu = s["cgroup_memory_stats"]["stat.anon"]
                    mc = s["cgroup_memory_stats"]["stat.file"]
                    mfail = s["cgroup_memory_stats"]["events.oom_kill"]
                cgroup_memory_stats.addRow(
                    [
                        s["timestamp"]["UTC"],
                        mu / divider,
                        mc / divider,
                        mfail,
                        mem_limit_bytes / divider,  # imposed limit of memory in this cgroup
                    ]
                )
            except KeyError as e:  # avoid crashing if a key is not present in the dictionary...
                print(f"Missing cgroup data while parsing {i}-th sample: {e}")
                n_invalid_samples += 1
                pass

        self.__print_data_loading_stats("cgroup memory", n_invalid_samples)

        # Produce the javascript:
        self.output_page.appendGoogleChart(
            GoogleChartsGraph(
                data=cgroup_memory_stats,  # Data
                graph_title="Used memory in " + unit + " (from memory cgroup)",
                button_label="Memory",
                y_axes_titles=[unit, "Alloc Failures"],
                graph_source=GRAPH_SOURCE_DATA_CGROUP,
                stack_state=False,
                columns_for_2nd_yaxis=["Alloc Failures"],
            )
        )

        return

    def generate_baremetal_avg_load(self):
        #
        # MAIN LOOP
        # Process JSON sample and build Google Chart-compatible Javascript variable
        # See https://developers.google.com/chart/interactive/docs/reference
        #

        num_baremetal_cpus = 1
        if "lscpu" in self.jheader:
            num_baremetal_cpus = int(self.jheader["lscpu"]["cpus"])

        load_avg_stats = GoogleChartsTimeSeries(["Timestamp", "LoadAvg (1min)", "LoadAvg (5min)", "LoadAvg (15min)"])
        for i, s in enumerate(self.jdata):
            if i == 0:
                continue  # skip first sample

            #
            # See https://linux.die.net/man/5/proc
            # and https://blog.appsignal.com/2018/03/28/understanding-system-load-and-load-averages.html
            #
            # "The load of a system is essentially the number of processes active at any given time.
            #  When idle, the load is 0. When a process starts, the load is incremented by 1.
            #  A terminating process decrements the load by 1. Besides running processes,
            #  any process that's queued up is also counted. So, when one process is actively using the CPU,
            #  and two are waiting their turn, the load is 3."
            #  ...
            # "Generally, single-core CPU can handle one process at a time. An average load of 1.0 would mean
            #  that one core is busy 100% of the time. If the load average drops to 0.5, the CPU has been idle
            #  for 50% of the time."

            # since kernel reports a percentage in range [0-n], where n= number of cores,
            # we remap that in range [0-100%]

            load_avg_stats.addRow(
                [
                    s["timestamp"]["UTC"],
                    100 * float(s["proc_loadavg"]["load_avg_1min"]) / num_baremetal_cpus,
                    100 * float(s["proc_loadavg"]["load_avg_5min"]) / num_baremetal_cpus,
                    100 * float(s["proc_loadavg"]["load_avg_15min"]) / num_baremetal_cpus,
                ]
            )

        # Produce the javascript:
        self.output_page.appendGoogleChart(
            GoogleChartsGraph(
                data=load_avg_stats,  # Data
                graph_title="Average Load " + " (from baremetal stats)",
                button_label="Average Load",
                y_axes_titles=["Load (%)"],
                graph_source=GRAPH_SOURCE_DATA_BAREMETAL,
                stack_state=False,
            )
        )
        return

    def generate_monitoring_summary(self):
        monitoring_summary = [
            # ( "User:", self.jheader["cmonitor"]["username"] ),   # not really useful
            ("Collected:", self.jheader["cmonitor"]["collecting"].replace(",", ", ")),
            # ( "Started sampling at:", self.sample_template["timestamp"]["datetime"] + " (Local)" ),   # not really useful
            ("Started sampling at:", self.jdata[0]["timestamp"]["UTC"] + " (UTC)"),
            ("Samples:", str(len(self.jdata))),
            ("Sampling Interval (s):", str(self.jheader["cmonitor"]["sample_interval_seconds"])),
            (
                "Total time sampled (hh:mm:ss):",
                str(datetime.timedelta(seconds=self.jheader["cmonitor"]["sample_interval_seconds"] * len(self.jdata))),
            ),
            ("Version (cmonitor_collector):", self.jheader["cmonitor"]["version"]),
            ("Version (cmonitor_chart):", VERSION_STRING),
        ]
        self.output_page.appendHtmlTable("Monitoring Summary", monitoring_summary)

    def __generate_monitored_summary_with_cpus(self, logical_cpus_indexes):
        # NOTE: unfortunately some useful information like:
        #        - RAM memory model/speed
        #        - Disk model/speed
        #        - NIC model/speed
        #       will not be available from inside a container, which is where cmonitor_collector usually runs...
        #       so we mostly show CPU stats:
        all_disks = []
        if "disks" in self.sample_template:
            all_disks = self.sample_template["disks"].keys()
        all_netdevices = []
        if "network_interfaces" in self.sample_template:
            all_netdevices = self.sample_template["network_interfaces"].keys()

        cpu_model = "Unknown"
        bogomips = "Unknown"
        if "lscpu" in self.jheader:
            cpu_model = self.jheader["lscpu"]["model_name"]
            bogomips = self.jheader["lscpu"]["bogomips"]

        monitored_summary = [
            ("Hostname:", self.jheader["identity"]["hostname"]),
            ("OS:", self.jheader["os_release"]["pretty_name"]),
            ("CPU:", cpu_model),
            ("BogoMIPS:", bogomips),
            ("Monitored CPUs:", str(len(logical_cpus_indexes))),
            ("Monitored Disks:", str(len(all_disks))),
            ("Monitored Network Devices:", str(len(all_netdevices))),
        ]
        return monitored_summary

    def generate_monitored_summary(self):
        if len(self.baremetal_logical_cpus_indexes) > 0:
            self.output_page.appendHtmlTable(
                "Monitored System Summary",
                self.__generate_monitored_summary_with_cpus(self.baremetal_logical_cpus_indexes),
            )
        elif len(self.cgroup_logical_cpus_indexes) > 0:
            self.output_page.appendHtmlTable(
                "Monitored System Summary",
                self.__generate_monitored_summary_with_cpus(self.cgroup_logical_cpus_indexes),
            )

    def generate_about_this(self):
        about_this = [
            ("Zoom:", "use left-click and drag"),
            ("Reset view:", "use right-click"),
            ("Generated by", '<a href="https://github.com/f18m/cmonitor">cmonitor</a>'),
        ]
        self.output_page.appendHtmlTable("About this", about_this, div_id="bottom_about_div")


# =======================================================================================================
# MAIN SCRIPT PREPARE DATA
# =======================================================================================================


def load_json_data(infile):
    """
    This function is able to read both JSON files produced by cmonitor_collector and
    compressed JSON.GZ files.
    Moreover this function is also tolerant to unfinished JSON files (i.e. files in which
    cmonitor_collector is still appending data).
    Finally it also performs very basic validation of JSON structure.
    Returns the JSON structure of the document.

    This function is shared between cmonitor_chart.py and cmonitor_statistics.py
    """
    # read the raw .json as text
    try:
        if infile[-8:] == ".json.gz":
            print("Loading gzipped JSON file %s" % infile)
            f = gzip.open(infile, "rb")
            text = f.read()
            f.close()

            # in Python 3.5 the gzip returns a sequence of "bytes" and not a "str"
            if isinstance(text, bytes):
                text = text.decode("utf-8")
        else:
            print("Loading JSON file %s" % infile)
            f = open(infile, "r")
            text = f.read()
            f.close()
    except OSError as err:
        print("Error while opening input JSON file '%s': %s" % (infile, err))
        sys.exit(1)

    # Convert the text to json and extract the stats
    try:
        entry = json.loads(text)  # convert text to JSON
    except json.decoder.JSONDecodeError as e:
        # fix up the end of the file if it is not complete
        if text[-1] == ",":
            # try removing last comma
            text[-1] = " "
        # add the closure of the "samples" array and JSON end-of-object
        entry = json.loads(text + "]}")

    try:
        jheader = entry["header"]
        jdata = entry["samples"]  # removes outer parts so we have a list of snapshot dictionaries
    except:
        print("Unexpected JSON format. Aborting.")
        sys.exit(1)

    # Check if cmonitor_collector version used differs or not:
    try:
        cmonitor_collector_version = jheader["cmonitor"]["version"]
        my_major = VERSION_STRING.split(".")[0]
        cmonitor_collector_major = cmonitor_collector_version.split(".")[0]
        if cmonitor_collector_major != my_major:
            print(
                f"ERROR: the input JSON file has been generated by cmonitor_collector v{cmonitor_collector_version}, which has a different MAJOR version compared to this tool which is v{VERSION_STRING}."
            )
            sys.exit(10)
        elif cmonitor_collector_version != VERSION_STRING:
            print(
                f"WARNING: the input JSON file has been generated by cmonitor_collector v{cmonitor_collector_version}, different than the version of this tool which is v{VERSION_STRING}."
            )
    except KeyError:
        pass
    return entry


def main_process_file(cfg):
    infile = cfg["input_json"]
    outfile = config["output_html"]
    start_time = time.time()

    # load the JSON
    entry = load_json_data(infile)
    jheader = entry["header"]
    jdata = entry["samples"]

    # load some basic fields from the JSON
    monitored_system = "Unknown"
    if "identity" in jheader:
        if "hostname" in jheader["identity"]:
            monitored_system = "host " + jheader["identity"]["hostname"]
    if "custom_metadata" in jheader:
        if "cmonitor_chart_name" in jheader["custom_metadata"]:
            monitored_system = jheader["custom_metadata"]["cmonitor_chart_name"]

    cgroupName = "None"
    if "cgroup_config" in jheader and "name" in jheader["cgroup_config"]:
        cgroupName = jheader["cgroup_config"]["name"]
    if "custom_metadata" in jheader:
        if "cmonitor_chart_name" in jheader["custom_metadata"]:
            cgroupName = "docker/" + jheader["custom_metadata"]["cmonitor_chart_name"]

    sample_template = jdata[0]
    if verbose:
        print("Found %d data samples" % len(jdata))

    print("Opening output file %s" % outfile)
    out_doc = HtmlOutputPage(outfile, "Monitoring data for " + monitored_system)
    graph_generator = CMonitorGraphGenerator(out_doc, jheader, jdata)

    # HTML HEAD -- all of these functions below generate JS code to be put inside the <HEAD>
    graph_generator.output_page.startHtmlHead()

    # baremetal stats:
    graph_generator.generate_baremetal_cpus()
    graph_generator.generate_baremetal_memory()
    graph_generator.generate_baremetal_network_traffic()
    graph_generator.generate_baremetal_disks_io()
    graph_generator.generate_baremetal_avg_load()

    # cgroup stats:
    graph_generator.generate_cgroup_cpus()
    graph_generator.generate_cgroup_memory()
    graph_generator.generate_cgroup_topN_procs(config["top_scorer"], config["thread_filter"])
    graph_generator.generate_cgroup_network_traffic()

    graph_generator.generate_config_viewer()

    # HTML BODY -- now we start actual HTML body which are just a few tables with buttons
    #              that invoke the JS code produced earlier inside the <HEAD>
    graph_generator.output_page.startHtmlBody(cgroupName, monitored_system)

    graph_generator.generate_monitoring_summary()
    graph_generator.generate_monitored_summary()
    graph_generator.generate_about_this()

    graph_generator.output_page.endHtmlBody()

    end_time = time.time()
    print("Completed processing of input JSON file of %d samples in %.3fsec. HTML output file is ready." % (len(jdata), end_time - start_time))


# =======================================================================================================
# MAIN
# =======================================================================================================


def usage():
    """Provides commandline usage"""
    print("cmonitor_chart version {}".format(VERSION_STRING))
    print("Utility to post-process data recorded by 'cmonitor_collector' and")
    print("create a self-contained HTML file for visualizing that data.")
    print("Typical usage:")
    print("  %s --input=output_from_cmonitor_collector.json [--output=myreport.html]" % sys.argv[0])
    print("Required parameters:")
    print("  -i, --input=<file.json>    The JSON file to analyze.")
    print("Main options:")
    print("  -h, --help                 (this help)")
    print("  -v, --verbose              Be verbose.")
    print("  -o, --output=<file.html>   The name of the output HTML file.")
    print("  -u, --utc                  Plot data using UTC timestamps instead of local timezone.")
    print("  -t, --top-scorer=N         Plot the N most-CPU-hungry processes/threads. Default is 20. Zero means plot all.")
    print("  -f, --filter=<tname>       Plot only the threads filtered by the selected substring . Default is None.")
    print("      --version              Print version and exit.")
    sys.exit(0)


def parse_command_line():
    """Parses the command line and returns the configuration as dictionary object."""
    try:
        opts, remaining_args = getopt.getopt(
            sys.argv[1:],
            "hvvut",
            [
                "help",
                "verbose",
                "version",
                "output=",
                "input=",
                "utc",
                "top-scorer=",
                "filter=",
            ],
        )
    except getopt.GetoptError as err:
        # print help information and exit:
        print(str(err))  # will print something like "option -a not recognized"
        usage()  # will exit program

    global verbose
    global g_datetime
    input_json = ""
    output_html = ""
    top_scorer = 20
    thread_filter = ""
    for o, a in opts:
        if o in ("-i", "--input"):
            input_json = a
        elif o in ("-o", "--output"):
            output_html = a
        elif o in ("-h", "--help"):
            usage()
        elif o in ("-v", "--verbose"):
            verbose = True
        elif o in ("-u", "--utc"):
            # instead of default 'datetime' which means local timezone
            g_datetime = "UTC"
        elif o in ("-t", "--top-scorer"):
            try:
                top_scorer = int(a)
            except:
                print("Invalid argument for --top-scorer. Provide a number.")
                sys.exit(1)
        elif o in ("-f", "--filter"):
            thread_filter = a
        elif o in ("--version"):
            print("{}".format(VERSION_STRING))
            sys.exit(0)
        else:
            assert False, "unhandled option " + o + a

    if input_json == "":
        print("Please provide --input option (it is a required option)")
        sys.exit(os.EX_USAGE)

    if output_html == "":
        if input_json[-8:] == ".json.gz":
            output_html = input_json[:-8] + ".html"
        elif input_json[-5:] == ".json":
            output_html = input_json[:-5] + ".html"
        else:
            output_html = input_json + ".html"

    abs_input_json = input_json
    if not os.path.isabs(input_json):
        abs_input_json = os.path.join(os.getcwd(), input_json)

    return {
        "input_json": input_json,
        "output_html": output_html,
        "top_scorer": top_scorer,
        "thread_filter": thread_filter,
    }


# =======================================================================================================
# MAIN
# =======================================================================================================

if __name__ == "__main__":
    config = parse_command_line()
    main_process_file(config)
